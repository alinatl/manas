{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfd2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==3.1.0\n",
    "import torch\n",
    "import transformers\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813c4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c00fbb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
    "from conllu import parse\n",
    "from conllu import parse_tree\n",
    "import pandas as pd\n",
    "from razdel import tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from conllu.models import TokenList, Token\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import pickle\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "470a9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def get_align(src, tgt_idx):\n",
    "\n",
    "    tgt = ' '.join(list(tgt_idx.keys()))\n",
    "\n",
    "    sent_src, sent_tgt = src.strip().split(), tgt.strip().split()\n",
    "    token_src, token_tgt = [tokenizer.tokenize(word) for word in sent_src], [tokenizer.tokenize(word) for word in sent_tgt]\n",
    "    wid_src, wid_tgt = [tokenizer.convert_tokens_to_ids(x) for x in token_src], [tokenizer.convert_tokens_to_ids(x) for x in token_tgt]\n",
    "    ids_src, ids_tgt = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids'], tokenizer.prepare_for_model(list(itertools.chain(*wid_tgt)), return_tensors='pt', truncation=True, model_max_length=tokenizer.model_max_length)['input_ids']\n",
    "    sub2word_map_src = []\n",
    "    for i, word_list in enumerate(token_src):\n",
    "        sub2word_map_src += [i for x in word_list]\n",
    "    sub2word_map_tgt = []\n",
    "    for i, word_list in enumerate(token_tgt):\n",
    "        sub2word_map_tgt += [i for x in word_list]\n",
    "\n",
    "    # alignment\n",
    "    align_layer = 8\n",
    "    threshold = 0.9\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
    "        out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
    "        dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
    "        softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
    "        softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
    "        softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
    "\n",
    "    align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
    "    align_words = set()\n",
    "    for i, j in align_subwords:\n",
    "        align_words.add( (sub2word_map_src[i], sub2word_map_tgt[j]) )\n",
    "\n",
    "\n",
    "    if len(list(align_words)) == 0:\n",
    "        return random.choice(list(tgt_idx.keys()))\n",
    "    else:\n",
    "        for i, j in sorted(align_words):\n",
    "            return sent_tgt[j]\n",
    "    \n",
    "\n",
    "# get_align(tgt, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a95575b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synthesized_sent:\n",
    "    def __init__(self, sent_source, sent_target):\n",
    "        \n",
    "        self.sent_source = sent_source\n",
    "        self.sent_target = sent_target\n",
    "        \n",
    "        self.rows_source = self._split_rows(self.sent_source)\n",
    "        self.rows_target = self._split_rows(self.sent_target)\n",
    "        self.sent_synt = TokenList([])\n",
    "        self.synt = TokenList([])\n",
    "        self.synt_target = TokenList([])\n",
    "        self.query = []\n",
    "        self.clausal_tags = ['root', 'parataxis', 'csubj', 'xcomp', 'ccomp', 'advcl', 'acl', 'conj']\n",
    "        self.POS_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'VERB', 'X']\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _split_rows(self, sentence):\n",
    "        \n",
    "        \"\"\"split the tree on rows using metadata info\"\"\"\n",
    "        \n",
    "        list_rows = [x.split() for x in re.split('\\d{4}\\s', sentence.metadata['text'])[1:]]\n",
    "        index_word = 0\n",
    "        sent_tokens = []\n",
    "\n",
    "        for row in list_rows:\n",
    "            row_heads = []\n",
    "            row_id = []\n",
    "            row_tokens = TokenList([])\n",
    "            for word in row:\n",
    "                row_tokens.append(Token(id = sentence[index_word]['id'], \n",
    "                            form = sentence[index_word]['form'], \n",
    "                            lemma = sentence[index_word]['lemma'],\n",
    "                            upos = sentence[index_word]['upos'],\n",
    "                            head = sentence[index_word]['head'],\n",
    "                            deprel = sentence[index_word]['deprel']))\n",
    "                index_word += 1\n",
    "\n",
    "            sent_tokens.append(row_tokens)\n",
    "        return sent_tokens\n",
    "\n",
    "    \n",
    "    \n",
    "    def predict_deprel(self):\n",
    "        \n",
    "        \"\"\"predict tag deprel using the pretrained model CRF\"\"\"\n",
    "        \n",
    "        train_sents = []\n",
    "        train_sents.append((trans2format(self.sent_target.filter(id=lambda x: type(x) is int), self.sent_source)))\n",
    "        X_train = [sent2features(s) for s in train_sents]\n",
    "        return crf.predict(X_train)[0]\n",
    "\n",
    "    \n",
    "            \n",
    "    def fill_deprel(self):\n",
    "        \n",
    "        \"\"\"fills deprel tags in the tree\"\"\"\n",
    "        \n",
    "        tags = self.predict_deprel()\n",
    "        for i, tok in enumerate(self.sent_target.filter(id=lambda x: type(x) is int)):\n",
    "            tok['pred_deprel'] = tags[i]            \n",
    "\n",
    "     \n",
    "\n",
    "    def fill_synt_sent(self, word_rus):\n",
    "        \n",
    "        \"\"\"fills syntenized kyrgyz tree self.synt_target\"\"\"\n",
    "\n",
    "        for id_row, row in enumerate(self.rows_source):\n",
    "            if list(row.filter(id = word_rus['id'])) != []:\n",
    "                tgt = {}\n",
    "                list_upos = {x['form']: x['id'] for x in self.rows_target[id_row].filter(upos = word_rus['upos']).filter(deprel = word_rus['deprel'])}\n",
    "\n",
    "                index_in_row = [y['id'] for y in self.rows_target[id_row]]\n",
    "                list_deprel = {x['form']: x['id'] for x in self.sent_target.filter(pred_deprel = word_rus['deprel']) if x['id'] in index_in_row}\n",
    "                \n",
    "                tgt.update(list_upos)\n",
    "                tgt.update(list_deprel)\n",
    "\n",
    "\n",
    "                if len(tgt.keys()) != 0:\n",
    "                    if len(tgt.keys()) > 1:\n",
    "                        aligned_word = get_align(word_rus['form'], tgt)\n",
    "                    elif len(tgt.keys()) == 1:\n",
    "                        aligned_word = list(tgt.keys())[0]\n",
    "\n",
    "                    self.synt_target.append(Token(rus_deprel = word_rus['deprel'],\n",
    "                                                  aligned_word_form = aligned_word,\n",
    "                                                  id_in_kyr_sent = tgt[aligned_word], \n",
    "                                                  id_rus = word_rus['id'],\n",
    "                                                  id_head_rus = word_rus['head']\n",
    "                    ))\n",
    "                    \n",
    "        \n",
    "    \n",
    "    def parse_rus_sent(self, word):\n",
    "        \n",
    "        \"\"\"itterates russian tree and fills synt kyrgyz tree\"\"\"\n",
    "        \n",
    "        if word.children != None:\n",
    "            self.query += word.children\n",
    "\n",
    "        self.fill_synt_sent(word.token)\n",
    "        if self.query != []:\n",
    "            \n",
    "            return self.parse_rus_sent(self.query.pop(0))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def head_renumerate(self):\n",
    "        for tok in self.synt.filter(id=lambda x: type(x) is int):\n",
    "\n",
    "            if tok['predicted'] == 'root':\n",
    "                self.synt[tok['id']-1]['head'] = 0\n",
    "            if tok['rus_head'] != '_':\n",
    "                for word_match in self.synt.filter(rus_word_id = tok['rus_head']).filter(id=lambda x: type(x) is int):\n",
    "\n",
    "                    self.synt[tok['id']-1]['head'] = word_match['id']\n",
    "    \n",
    "\n",
    "    def find_head_of_the_clause(self, token):\n",
    "        for row in self.rows_target:\n",
    "            list_ids = [x['id'] for x in row.filter(id=lambda x: type(x) is int)]\n",
    "            if token['id'] in list_ids:\n",
    "                for id_tok in list_ids:\n",
    "                    if self.sent_target.filter(id=id_tok)[0]['pred_deprel'] in self.clausal_tags:\n",
    "                        return id_tok\n",
    "                \n",
    "                \n",
    "    def easy_rules(self):\n",
    "        \n",
    "        \"\"\"rules for case, appos, cop, det\"\"\"\n",
    "        \n",
    "        for tok in self.sent_target.filter(id=lambda x: type(x) is int):\n",
    "            if tok['pred_deprel'] == 'case':\n",
    "                tok['pred_head'] = int(tok['id']) - 1\n",
    "                \n",
    "            if tok['pred_deprel'] == 'cop':\n",
    "                tok['pred_head'] = int(tok['id']) - 1\n",
    "                \n",
    "            elif tok['pred_deprel'] == 'appos':\n",
    "                tok['pred_head'] = int(tok['id']) - 1\n",
    "                \n",
    "            elif tok['pred_deprel'] == 'aux':\n",
    "                tok['pred_head'] = int(tok['id']) - 1\n",
    "                \n",
    "            elif tok['pred_deprel'] == 'det' and tok['pred_head'] == '_':\n",
    "                for row in self.rows_target: \n",
    "                    list_id_row = [x['id'] for x in row]\n",
    "                    if tok['id'] in list_id_row:\n",
    "                        break\n",
    "                id_pred = self.sent_target.filter(id=lambda x: x in list_id_row).filter(upos = 'NOUN')\n",
    "                if len(id_pred) != 0:\n",
    "                    tok['pred_head'] = id_pred[0]['id']\n",
    "                    \n",
    "            elif tok['pred_deprel'] == 'root' and tok['pred_head'] != 0:\n",
    "                tok['pred_head'] = 0\n",
    "                \n",
    "            elif tok['pred_deprel'] == 'punct':\n",
    "                head_clause = self.find_head_of_the_clause(tok)\n",
    "                \n",
    "                if head_clause != None:\n",
    "                    tok['pred_head'] = head_clause\n",
    "                    \n",
    "\n",
    "                    \n",
    "    def add_dep_to_target(self):\n",
    "        for tok in self.synt_target:\n",
    "            for x in self.synt_target.filter(id_head_rus = tok['id_rus']):\n",
    "                self.sent_target[x['id_in_kyr_sent'] - 1]['pred_head'] = tok['id_in_kyr_sent']\n",
    "#                 self.sent_target[x['id_in_kyr_sent'] - 1]['pred_deprel'] = tok['rus_deprel']\n",
    "\n",
    "        for i, x in enumerate(self.sent_target):\n",
    "            if 'pred_head' not in self.sent_target[i]:\n",
    "                self.sent_target[i]['pred_head'] = '_'\n",
    "                \n",
    "        self.easy_rules()\n",
    "        \n",
    "                \n",
    "    def cal_uas(self):\n",
    "        score_uas = 0\n",
    "        score_las = 0\n",
    "        for i, tok in enumerate(self.sent_target.filter(id=lambda x: type(x) is int)):\n",
    "            if tok['pred_head'] == tok['head'] and tok['pred_deprel'] == tok['deprel']:\n",
    "                score_uas += 1\n",
    "            elif tok['pred_head'] == tok['head']:\n",
    "                score_las += 1\n",
    "        score_uas = score_uas/len(self.sent_target.filter(id=lambda x: type(x) is int))\n",
    "        score_las = score_las/len(self.sent_target.filter(id=lambda x: type(x) is int))\n",
    "        return score_uas, score_las\n",
    "\n",
    "                            \n",
    "    def fill_gapes(self):\n",
    "        root_id = self.sent_target.to_tree().token['id']\n",
    "        for x in self.sent_target.filter(pred_head = '_').filter(id=lambda x: type(x) is int):\n",
    "            x['pred_head'] = root_id\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1522e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "\n",
    "    word = sent[i][0]\n",
    "    lemma = sent[i][1]\n",
    "    postag = sent[i][2]\n",
    "    set_deprel = [x['deprel'] for x in sent[i][4]]\n",
    "#     print(set_deprel)\n",
    "#     id_row = sent[5]\n",
    "    \n",
    "    rus_deprel = [x['deprel'] for x in sent[i][4].filter(upos = postag)]\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "    features = {\n",
    "#         'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'len(word)': len(word),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'postag': postag,\n",
    "        'lemma.lower()': lemma.lower(),\n",
    "#         'id_row': id_row,\n",
    "        'id_word_in_row': i,\n",
    "        'lemma.lower() == word.lower()':lemma.lower() == word.lower()\n",
    "    }\n",
    "    \n",
    "\n",
    "    feat_transl = {str(i)+'_rus_deprel': x for i, x in enumerate(rus_deprel)}\n",
    "    features.update(feat_transl)\n",
    "\n",
    "    \n",
    "    if i > 1:\n",
    "        word1 = sent[i-2][0]\n",
    "        lemma1 = sent[i-2][1]\n",
    "        postag1 = sent[i-2][2]\n",
    "        features.update({\n",
    "            '-2:word.lower()': word1.lower(),\n",
    "            '-2:len(word)': len(word1),\n",
    "            '-2:postag': postag1,\n",
    "            '-2:lemma.lower()': lemma1.lower(),\n",
    "            '-2:id_word_in_row': i,\n",
    "            '-2:lemma.lower() == word.lower()': lemma1.lower() == word1.lower()\n",
    "        })\n",
    "\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        lemma1 = sent[i-1][1]\n",
    "        postag1 = sent[i-1][2]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:len(word)': len(word1),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:lemma.lower()': lemma1.lower(),\n",
    "            '-1:id_word_in_row': i,\n",
    "            '-1:lemma.lower() == word.lower()': lemma1.lower() == word1.lower()\n",
    "        })\n",
    "        \n",
    "    if i < len(sent)-2:\n",
    "        word1 = sent[i+2][0]\n",
    "        lemma1 = sent[i+2][1]\n",
    "        postag1 = sent[i+2][2]\n",
    "        features.update({\n",
    "            '+2:word.lower()': word1.lower(),\n",
    "            '+2:len(word)': len(word1),\n",
    "            '+2:postag': postag1,\n",
    "            '+2:lemma.lower()' : lemma1.lower(),\n",
    "            '+2:id_word_in_row': i,\n",
    "            '+2:lemma.lower() == word.lower()':lemma1.lower() == word1.lower()\n",
    "        })\n",
    "            \n",
    "            \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        lemma1 = sent[i+1][1]\n",
    "        postag1 = sent[i+1][2]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:len(word)': len(word1),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:lemma.lower()' : lemma1.lower(),\n",
    "            '+1:id_word_in_row': i,\n",
    "            '+1:lemma.lower() == word.lower()':lemma1.lower() == word1.lower()\n",
    "        })\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, lemma, postag, label, set_deprel in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, lemma, postag, label, set_deprel in sent]\n",
    "\n",
    "\n",
    "\n",
    "def trans2format(sent_ky, sent_ru):\n",
    "    new_sent = []   \n",
    "    for word in sent_ky:\n",
    "        new_sent.append((word['form'], word['lemma'], word['upos'], word['deprel'], sent_ru))\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8acf98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузить данные\n",
    "\n",
    "folder = 'path/to_dir/'\n",
    "with open(folder + 'kyr.conllu', 'r', encoding = 'utf-8') as f:\n",
    "    file_ky = parse(f.read())\n",
    "\n",
    "\n",
    "with open(folder + 'rus.conllu', 'r', encoding = 'utf-8') as f:\n",
    "    file_ru = parse(f.read())\n",
    "\n",
    "map_ky_ru = {}\n",
    "for ru_sent in file_ru:\n",
    "    ky_sent = [x for x in file_ky if x.metadata['sent_id'] == ru_sent.metadata['sent_id']]\n",
    "    if len(ky_sent) == 1:\n",
    "        map_ky_ru[ru_sent.metadata['sent_id']] = {'ru': ru_sent, 'ky':ky_sent[0]}\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# map_ky_ru['5134']['ky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1c5c193c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|███████████████████████████████████████████| 393/393 [00:00<00:00, 3119.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 33468\n",
      "Seconds required: 0.060\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.10  loss=16364.03 active=33059 feature_norm=0.12\n",
      "Iter 2   time=0.02  loss=15688.10 active=33042 feature_norm=0.13\n",
      "Iter 3   time=0.02  loss=15053.14 active=33150 feature_norm=0.21\n",
      "Iter 4   time=0.02  loss=14553.51 active=33108 feature_norm=0.28\n",
      "Iter 5   time=0.02  loss=13933.78 active=33226 feature_norm=0.43\n",
      "Iter 6   time=0.02  loss=13325.60 active=33231 feature_norm=0.55\n",
      "Iter 7   time=0.02  loss=12625.46 active=33094 feature_norm=0.76\n",
      "Iter 8   time=0.02  loss=12136.37 active=33121 feature_norm=0.96\n",
      "Iter 9   time=0.02  loss=11683.83 active=33244 feature_norm=1.16\n",
      "Iter 10  time=0.02  loss=11267.54 active=33289 feature_norm=1.39\n",
      "Iter 11  time=0.02  loss=10702.17 active=33132 feature_norm=2.02\n",
      "Iter 12  time=0.02  loss=10333.22 active=33176 feature_norm=2.44\n",
      "Iter 13  time=0.02  loss=10053.48 active=33166 feature_norm=2.96\n",
      "Iter 14  time=0.02  loss=9581.60  active=33017 feature_norm=4.28\n",
      "Iter 15  time=0.02  loss=8914.78  active=32828 feature_norm=6.16\n",
      "Iter 16  time=0.02  loss=8152.91  active=32841 feature_norm=7.64\n",
      "Iter 17  time=0.02  loss=7590.54  active=32776 feature_norm=9.22\n",
      "Iter 18  time=0.02  loss=6929.85  active=32656 feature_norm=10.30\n",
      "Iter 19  time=0.02  loss=6213.94  active=32126 feature_norm=12.65\n",
      "Iter 20  time=0.02  loss=5757.30  active=31733 feature_norm=14.26\n",
      "Iter 21  time=0.02  loss=5366.21  active=31512 feature_norm=15.60\n",
      "Iter 22  time=0.02  loss=5020.67  active=31316 feature_norm=17.03\n",
      "Iter 23  time=0.02  loss=4541.72  active=29305 feature_norm=19.82\n",
      "Iter 24  time=0.02  loss=4239.20  active=29047 feature_norm=22.78\n",
      "Iter 25  time=0.02  loss=3884.36  active=29028 feature_norm=24.38\n",
      "Iter 26  time=0.02  loss=3644.48  active=28863 feature_norm=26.41\n",
      "Iter 27  time=0.02  loss=3272.16  active=28672 feature_norm=32.23\n",
      "Iter 28  time=0.02  loss=2967.90  active=28260 feature_norm=35.17\n",
      "Iter 29  time=0.02  loss=2774.87  active=28409 feature_norm=37.18\n",
      "Iter 30  time=0.02  loss=2442.57  active=27568 feature_norm=42.84\n",
      "Iter 31  time=0.02  loss=2180.00  active=27221 feature_norm=48.14\n",
      "Iter 32  time=0.02  loss=1993.84  active=27361 feature_norm=52.02\n",
      "Iter 33  time=0.02  loss=1864.08  active=27176 feature_norm=55.48\n",
      "Iter 34  time=0.02  loss=1749.63  active=26108 feature_norm=57.71\n",
      "Iter 35  time=0.02  loss=1685.07  active=25903 feature_norm=58.58\n",
      "Iter 36  time=0.02  loss=1631.54  active=25765 feature_norm=59.39\n",
      "Iter 37  time=0.02  loss=1602.49  active=25590 feature_norm=59.76\n",
      "Iter 38  time=0.02  loss=1580.32  active=25581 feature_norm=59.80\n",
      "Iter 39  time=0.02  loss=1558.59  active=25450 feature_norm=60.11\n",
      "Iter 40  time=0.02  loss=1545.62  active=24147 feature_norm=60.57\n",
      "Iter 41  time=0.02  loss=1532.47  active=23648 feature_norm=60.88\n",
      "Iter 42  time=0.02  loss=1523.05  active=23487 feature_norm=61.19\n",
      "Iter 43  time=0.02  loss=1512.12  active=23113 feature_norm=61.53\n",
      "Iter 44  time=0.02  loss=1503.77  active=22929 feature_norm=61.68\n",
      "Iter 45  time=0.02  loss=1495.53  active=22739 feature_norm=61.78\n",
      "Iter 46  time=0.02  loss=1486.16  active=22306 feature_norm=61.84\n",
      "Iter 47  time=0.02  loss=1477.15  active=22045 feature_norm=61.79\n",
      "Iter 48  time=0.02  loss=1468.19  active=21734 feature_norm=61.76\n",
      "Iter 49  time=0.04  loss=1464.21  active=21511 feature_norm=61.72\n",
      "Iter 50  time=0.02  loss=1455.57  active=21392 feature_norm=61.75\n",
      "Iter 51  time=0.02  loss=1449.89  active=21098 feature_norm=61.76\n",
      "Iter 52  time=0.02  loss=1444.76  active=20616 feature_norm=61.86\n",
      "Iter 53  time=0.02  loss=1437.47  active=20678 feature_norm=61.86\n",
      "Iter 54  time=0.02  loss=1434.03  active=20573 feature_norm=61.88\n",
      "Iter 55  time=0.02  loss=1428.36  active=20235 feature_norm=61.93\n",
      "Iter 56  time=0.02  loss=1424.15  active=20063 feature_norm=61.91\n",
      "Iter 57  time=0.02  loss=1420.36  active=19994 feature_norm=61.90\n",
      "Iter 58  time=0.02  loss=1416.61  active=19874 feature_norm=61.83\n",
      "Iter 59  time=0.02  loss=1413.89  active=19647 feature_norm=61.69\n",
      "Iter 60  time=0.02  loss=1409.51  active=19697 feature_norm=61.67\n",
      "Iter 61  time=0.02  loss=1407.44  active=19626 feature_norm=61.61\n",
      "Iter 62  time=0.02  loss=1404.14  active=19364 feature_norm=61.50\n",
      "Iter 63  time=0.02  loss=1402.00  active=19403 feature_norm=61.46\n",
      "Iter 64  time=0.02  loss=1400.90  active=19400 feature_norm=61.45\n",
      "Iter 65  time=0.02  loss=1398.49  active=19232 feature_norm=61.40\n",
      "Iter 66  time=0.02  loss=1396.62  active=19066 feature_norm=61.33\n",
      "Iter 67  time=0.02  loss=1394.19  active=19066 feature_norm=61.30\n",
      "Iter 68  time=0.02  loss=1392.78  active=19010 feature_norm=61.26\n",
      "Iter 69  time=0.02  loss=1390.49  active=18818 feature_norm=61.19\n",
      "Iter 70  time=0.02  loss=1388.50  active=18757 feature_norm=61.13\n",
      "Iter 71  time=0.02  loss=1386.89  active=18740 feature_norm=61.09\n",
      "Iter 72  time=0.02  loss=1385.17  active=18628 feature_norm=61.03\n",
      "Iter 73  time=0.02  loss=1383.26  active=18487 feature_norm=60.96\n",
      "Iter 74  time=0.02  loss=1381.69  active=18410 feature_norm=60.91\n",
      "Iter 75  time=0.02  loss=1380.23  active=18382 feature_norm=60.87\n",
      "Iter 76  time=0.02  loss=1378.72  active=18337 feature_norm=60.83\n",
      "Iter 77  time=0.02  loss=1377.19  active=18226 feature_norm=60.79\n",
      "Iter 78  time=0.02  loss=1375.78  active=18178 feature_norm=60.77\n",
      "Iter 79  time=0.02  loss=1374.57  active=18127 feature_norm=60.74\n",
      "Iter 80  time=0.02  loss=1373.42  active=18021 feature_norm=60.72\n",
      "Iter 81  time=0.02  loss=1372.42  active=17953 feature_norm=60.70\n",
      "Iter 82  time=0.02  loss=1371.42  active=17923 feature_norm=60.68\n",
      "Iter 83  time=0.02  loss=1370.50  active=17887 feature_norm=60.65\n",
      "Iter 84  time=0.02  loss=1369.48  active=17833 feature_norm=60.62\n",
      "Iter 85  time=0.02  loss=1368.48  active=17802 feature_norm=60.59\n",
      "Iter 86  time=0.02  loss=1367.60  active=17768 feature_norm=60.58\n",
      "Iter 87  time=0.02  loss=1366.84  active=17756 feature_norm=60.56\n",
      "Iter 88  time=0.02  loss=1366.03  active=17733 feature_norm=60.54\n",
      "Iter 89  time=0.02  loss=1365.25  active=17687 feature_norm=60.52\n",
      "Iter 90  time=0.02  loss=1364.45  active=17585 feature_norm=60.49\n",
      "Iter 91  time=0.02  loss=1363.88  active=17575 feature_norm=60.47\n",
      "Iter 92  time=0.02  loss=1363.21  active=17576 feature_norm=60.46\n",
      "Iter 93  time=0.02  loss=1362.68  active=17527 feature_norm=60.44\n",
      "Iter 94  time=0.02  loss=1361.96  active=17465 feature_norm=60.43\n",
      "Iter 95  time=0.02  loss=1361.30  active=17440 feature_norm=60.42\n",
      "Iter 96  time=0.02  loss=1360.83  active=17407 feature_norm=60.41\n",
      "Iter 97  time=0.02  loss=1360.28  active=17390 feature_norm=60.40\n",
      "Iter 98  time=0.02  loss=1359.87  active=17369 feature_norm=60.39\n",
      "Iter 99  time=0.02  loss=1359.33  active=17318 feature_norm=60.37\n",
      "Iter 100 time=0.02  loss=1358.78  active=17274 feature_norm=60.35\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 2.231\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 17274 (33468)\n",
      "Number of active attributes: 9762 (17164)\n",
      "Number of active labels: 29 (29)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.011\n",
      "\n",
      "F1-score is : 75.6%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       1.00      1.00      1.00         5\n",
      "         acl       0.76      0.76      0.76        25\n",
      "       advcl       0.91      0.92      0.91        52\n",
      "      advmod       0.94      0.92      0.93        37\n",
      " advmod:emph       1.00      1.00      1.00         6\n",
      "        amod       0.79      0.92      0.85        25\n",
      "       appos       0.80      0.62      0.70        13\n",
      "         aux       0.86      0.94      0.90        32\n",
      "        case       0.89      0.80      0.84        10\n",
      "          cc       1.00      1.00      1.00         4\n",
      "       ccomp       0.92      0.61      0.73        18\n",
      "    compound       0.69      0.50      0.58        18\n",
      "        conj       0.75      0.54      0.63        39\n",
      "         cop       0.88      1.00      0.93         7\n",
      "         det       1.00      0.62      0.76        13\n",
      "   discourse       1.00      1.00      1.00         4\n",
      "       fixed       0.00      0.00      0.00         3\n",
      "        flat       0.00      0.00      0.00         1\n",
      "        nmod       0.57      0.35      0.43        23\n",
      "   nmod:poss       0.62      0.76      0.68        17\n",
      "       nsubj       0.83      0.88      0.86        67\n",
      "      nummod       0.88      0.88      0.88        16\n",
      "         obj       0.79      0.76      0.78        50\n",
      "         obl       0.93      0.93      0.93        58\n",
      "   parataxis       0.62      0.63      0.63        41\n",
      "       punct       1.00      1.00      1.00        94\n",
      "        root       1.00      0.91      0.96        94\n",
      "    vocative       1.00      0.33      0.50         3\n",
      "\n",
      "   micro avg       0.87      0.82      0.84       775\n",
      "   macro avg       0.80      0.74      0.76       775\n",
      "weighted avg       0.86      0.82      0.84       775\n",
      " samples avg       0.87      0.83      0.85       775\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# разбить на трейн и текст и обучить модель\n",
    "\n",
    "train_sents = []\n",
    "test_sents = []\n",
    "\n",
    "with open('test_ids.txt', 'r', encoding = 'utf-8') as f:\n",
    "    test_sents_id = sorted(f.read().split('\\n'))\n",
    "\n",
    "for i, ky_ru_sent in map_ky_ru.items():\n",
    "\n",
    "    syntenizer = Synthesized_sent(map_ky_ru[i]['ru'], map_ky_ru[i]['ky'])\n",
    "    if i in ['4059', '4647', '4698', '4858', '4902', '5064', '4729', '5452']:\n",
    "        pass\n",
    "    elif i not in test_sents_id:\n",
    "        train_sents.append((trans2format(syntenizer.sent_target.filter(id=lambda x: type(x) is int), syntenizer.sent_source)))\n",
    "    else:\n",
    "        test_sents.append((trans2format(syntenizer.sent_target.filter(id=lambda x: type(x) is int), syntenizer.sent_source)))\n",
    "\n",
    "            \n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "\n",
    "m = MultiLabelBinarizer().fit(y_test)\n",
    "\n",
    "print(\"F1-score is : {:.1%}\".format(f1_score(m.transform(y_test),\n",
    "         m.transform(y_pred),\n",
    "         average='macro')))\n",
    "print(classification_report(m.transform(y_test), m.transform(y_pred), target_names=m.classes_))\n",
    "\n",
    "labels = list(crf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17816c3",
   "metadata": {},
   "source": [
    "# Кроссвалидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef1a3a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|███████████████████████████████████████████| 393/393 [00:00<00:00, 3144.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 33468\n",
      "Seconds required: 0.062\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.078174\n",
      "c2: 0.116035\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.10  loss=16363.97 active=33078 feature_norm=0.12\n",
      "Iter 2   time=0.02  loss=15688.02 active=33091 feature_norm=0.13\n",
      "Iter 3   time=0.02  loss=15052.98 active=33176 feature_norm=0.21\n",
      "Iter 4   time=0.02  loss=14553.31 active=33106 feature_norm=0.28\n",
      "Iter 5   time=0.02  loss=13933.50 active=33261 feature_norm=0.43\n",
      "Iter 6   time=0.02  loss=13325.21 active=33235 feature_norm=0.55\n",
      "Iter 7   time=0.02  loss=12624.90 active=33118 feature_norm=0.76\n",
      "Iter 8   time=0.02  loss=12135.60 active=33157 feature_norm=0.96\n",
      "Iter 9   time=0.02  loss=11682.89 active=33258 feature_norm=1.16\n",
      "Iter 10  time=0.02  loss=11266.30 active=33286 feature_norm=1.39\n",
      "Iter 11  time=0.02  loss=10700.33 active=33166 feature_norm=2.03\n",
      "Iter 12  time=0.02  loss=10330.86 active=33214 feature_norm=2.44\n",
      "Iter 13  time=0.02  loss=10050.50 active=33225 feature_norm=2.96\n",
      "Iter 14  time=0.02  loss=9577.22  active=33092 feature_norm=4.28\n",
      "Iter 15  time=0.02  loss=8907.88  active=32962 feature_norm=6.17\n",
      "Iter 16  time=0.02  loss=8142.88  active=32985 feature_norm=7.66\n",
      "Iter 17  time=0.02  loss=7580.27  active=32963 feature_norm=9.21\n",
      "Iter 18  time=0.02  loss=6911.90  active=32877 feature_norm=10.30\n",
      "Iter 19  time=0.02  loss=6192.07  active=32333 feature_norm=12.67\n",
      "Iter 20  time=0.02  loss=5730.73  active=31697 feature_norm=14.32\n",
      "Iter 21  time=0.02  loss=5330.23  active=31629 feature_norm=15.66\n",
      "Iter 22  time=0.02  loss=4974.24  active=31513 feature_norm=17.05\n",
      "Iter 23  time=0.02  loss=4488.99  active=29418 feature_norm=20.01\n",
      "Iter 24  time=0.02  loss=4124.57  active=29149 feature_norm=23.06\n",
      "Iter 25  time=0.02  loss=3827.02  active=29104 feature_norm=24.88\n",
      "Iter 26  time=0.02  loss=3563.03  active=29012 feature_norm=26.76\n",
      "Iter 27  time=0.02  loss=3187.42  active=28849 feature_norm=31.27\n",
      "Iter 28  time=0.02  loss=2879.62  active=28269 feature_norm=36.62\n",
      "Iter 29  time=0.02  loss=2622.81  active=28276 feature_norm=38.79\n",
      "Iter 30  time=0.02  loss=2407.48  active=28084 feature_norm=41.98\n",
      "Iter 31  time=0.02  loss=2073.27  active=27005 feature_norm=51.92\n",
      "Iter 32  time=0.02  loss=1911.98  active=27256 feature_norm=53.93\n",
      "Iter 33  time=0.02  loss=1831.30  active=27350 feature_norm=54.99\n",
      "Iter 34  time=0.02  loss=1728.87  active=27326 feature_norm=56.87\n",
      "Iter 35  time=0.02  loss=1647.78  active=26694 feature_norm=58.75\n",
      "Iter 36  time=0.02  loss=1584.31  active=26455 feature_norm=59.27\n",
      "Iter 37  time=0.02  loss=1552.86  active=26446 feature_norm=59.42\n",
      "Iter 38  time=0.02  loss=1526.60  active=25972 feature_norm=59.54\n",
      "Iter 39  time=0.02  loss=1503.13  active=25611 feature_norm=59.45\n",
      "Iter 40  time=0.04  loss=1494.59  active=25545 feature_norm=59.59\n",
      "Iter 41  time=0.02  loss=1477.72  active=25327 feature_norm=59.81\n",
      "Iter 42  time=0.02  loss=1467.63  active=25193 feature_norm=60.08\n",
      "Iter 43  time=0.02  loss=1456.38  active=24782 feature_norm=60.59\n",
      "Iter 44  time=0.02  loss=1447.70  active=24469 feature_norm=60.89\n",
      "Iter 45  time=0.02  loss=1441.77  active=24380 feature_norm=61.00\n",
      "Iter 46  time=0.02  loss=1432.60  active=24159 feature_norm=61.25\n",
      "Iter 47  time=0.02  loss=1424.95  active=23920 feature_norm=61.37\n",
      "Iter 48  time=0.02  loss=1416.28  active=23688 feature_norm=61.45\n",
      "Iter 49  time=0.02  loss=1407.52  active=23430 feature_norm=61.45\n",
      "Iter 50  time=0.04  loss=1402.31  active=23235 feature_norm=61.38\n",
      "Iter 51  time=0.02  loss=1394.15  active=23093 feature_norm=61.31\n",
      "Iter 52  time=0.02  loss=1387.46  active=22951 feature_norm=61.22\n",
      "Iter 53  time=0.02  loss=1379.34  active=22778 feature_norm=61.09\n",
      "Iter 54  time=0.02  loss=1372.81  active=22516 feature_norm=61.00\n",
      "Iter 55  time=0.02  loss=1366.57  active=22356 feature_norm=60.89\n",
      "Iter 56  time=0.02  loss=1361.81  active=22189 feature_norm=60.81\n",
      "Iter 57  time=0.02  loss=1356.95  active=22111 feature_norm=60.76\n",
      "Iter 58  time=0.02  loss=1352.77  active=22059 feature_norm=60.69\n",
      "Iter 59  time=0.02  loss=1347.62  active=21824 feature_norm=60.60\n",
      "Iter 60  time=0.02  loss=1344.93  active=21595 feature_norm=60.55\n",
      "Iter 61  time=0.02  loss=1341.13  active=21561 feature_norm=60.53\n",
      "Iter 62  time=0.02  loss=1338.55  active=21476 feature_norm=60.50\n",
      "Iter 63  time=0.02  loss=1334.74  active=21205 feature_norm=60.47\n",
      "Iter 64  time=0.02  loss=1332.52  active=21155 feature_norm=60.44\n",
      "Iter 65  time=0.02  loss=1330.51  active=21109 feature_norm=60.40\n",
      "Iter 66  time=0.02  loss=1327.82  active=20975 feature_norm=60.34\n",
      "Iter 67  time=0.02  loss=1326.78  active=20867 feature_norm=60.27\n",
      "Iter 68  time=0.02  loss=1324.01  active=20883 feature_norm=60.26\n",
      "Iter 69  time=0.02  loss=1322.71  active=20854 feature_norm=60.23\n",
      "Iter 70  time=0.02  loss=1320.61  active=20567 feature_norm=60.14\n",
      "Iter 71  time=0.02  loss=1319.11  active=20656 feature_norm=60.13\n",
      "Iter 72  time=0.02  loss=1318.56  active=20680 feature_norm=60.12\n",
      "Iter 73  time=0.02  loss=1316.19  active=20441 feature_norm=60.07\n",
      "Iter 74  time=0.02  loss=1315.31  active=20394 feature_norm=60.02\n",
      "Iter 75  time=0.02  loss=1313.80  active=20401 feature_norm=60.00\n",
      "Iter 76  time=0.02  loss=1312.78  active=20352 feature_norm=59.97\n",
      "Iter 77  time=0.02  loss=1311.22  active=20247 feature_norm=59.92\n",
      "Iter 78  time=0.02  loss=1309.64  active=20135 feature_norm=59.85\n",
      "Iter 79  time=0.02  loss=1308.43  active=20128 feature_norm=59.83\n",
      "Iter 80  time=0.02  loss=1307.20  active=20132 feature_norm=59.79\n",
      "Iter 81  time=0.02  loss=1305.99  active=20078 feature_norm=59.75\n",
      "Iter 82  time=0.02  loss=1304.93  active=19998 feature_norm=59.72\n",
      "Iter 83  time=0.02  loss=1303.89  active=19931 feature_norm=59.70\n",
      "Iter 84  time=0.02  loss=1302.91  active=19892 feature_norm=59.67\n",
      "Iter 85  time=0.02  loss=1302.09  active=19856 feature_norm=59.66\n",
      "Iter 86  time=0.02  loss=1301.14  active=19821 feature_norm=59.63\n",
      "Iter 87  time=0.02  loss=1300.25  active=19788 feature_norm=59.61\n",
      "Iter 88  time=0.02  loss=1299.39  active=19728 feature_norm=59.58\n",
      "Iter 89  time=0.02  loss=1298.60  active=19723 feature_norm=59.56\n",
      "Iter 90  time=0.02  loss=1297.87  active=19682 feature_norm=59.54\n",
      "Iter 91  time=0.02  loss=1297.18  active=19663 feature_norm=59.53\n",
      "Iter 92  time=0.02  loss=1296.47  active=19613 feature_norm=59.52\n",
      "Iter 93  time=0.02  loss=1295.79  active=19591 feature_norm=59.51\n",
      "Iter 94  time=0.02  loss=1295.17  active=19564 feature_norm=59.51\n",
      "Iter 95  time=0.02  loss=1294.63  active=19522 feature_norm=59.50\n",
      "Iter 96  time=0.02  loss=1294.11  active=19485 feature_norm=59.50\n",
      "Iter 97  time=0.02  loss=1293.59  active=19435 feature_norm=59.50\n",
      "Iter 98  time=0.02  loss=1293.08  active=19414 feature_norm=59.49\n",
      "Iter 99  time=0.02  loss=1292.63  active=19376 feature_norm=59.49\n",
      "Iter 100 time=0.02  loss=1292.17  active=19347 feature_norm=59.49\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 2.261\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 19347 (33468)\n",
      "Number of active attributes: 10695 (17164)\n",
      "Number of active labels: 29 (29)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=True,\n",
    "    keep_tempfiles=None\n",
    ")\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "predictions = rs.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad970646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score is : 75.8%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       1.00      1.00      1.00         5\n",
      "         acl       0.76      0.76      0.76        25\n",
      "       advcl       0.91      0.92      0.91        52\n",
      "      advmod       0.94      0.92      0.93        37\n",
      " advmod:emph       1.00      1.00      1.00         6\n",
      "        amod       0.82      0.92      0.87        25\n",
      "       appos       0.80      0.62      0.70        13\n",
      "         aux       0.86      0.94      0.90        32\n",
      "        case       0.89      0.80      0.84        10\n",
      "          cc       1.00      1.00      1.00         4\n",
      "       ccomp       0.92      0.61      0.73        18\n",
      "    compound       0.75      0.50      0.60        18\n",
      "        conj       0.71      0.51      0.60        39\n",
      "         cop       0.88      1.00      0.93         7\n",
      "         det       1.00      0.62      0.76        13\n",
      "   discourse       1.00      1.00      1.00         4\n",
      "       fixed       0.00      0.00      0.00         3\n",
      "        flat       0.00      0.00      0.00         1\n",
      "        nmod       0.57      0.35      0.43        23\n",
      "   nmod:poss       0.62      0.76      0.68        17\n",
      "       nsubj       0.83      0.88      0.86        67\n",
      "      nummod       0.88      0.88      0.88        16\n",
      "         obj       0.84      0.76      0.80        50\n",
      "         obl       0.95      0.91      0.93        58\n",
      "   parataxis       0.65      0.63      0.64        41\n",
      "       punct       1.00      1.00      1.00        94\n",
      "        root       1.00      0.93      0.96        94\n",
      "    vocative       1.00      0.33      0.50         3\n",
      "\n",
      "   micro avg       0.88      0.82      0.85       775\n",
      "   macro avg       0.81      0.73      0.76       775\n",
      "weighted avg       0.87      0.82      0.84       775\n",
      " samples avg       0.88      0.83      0.85       775\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "m = MultiLabelBinarizer().fit(y_test)\n",
    "\n",
    "print(\"F1-score is : {:.1%}\".format(f1_score(m.transform(y_test),\n",
    "         m.transform(y_pred),\n",
    "         average='macro')))\n",
    "print(classification_report(m.transform(y_test), m.transform(y_pred), target_names=m.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1df738df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "5.347810 advmod   postag:ADV\n",
      "4.918523 det      postag:DET\n",
      "4.916751 aux      postag:AUX\n",
      "4.666810 _        postag:_\n",
      "4.611634 amod     postag:ADJ\n",
      "4.049232 nummod   postag:NUM\n",
      "3.611629 obl      word[-2:]:га\n",
      "3.419329 case     postag:ADP\n",
      "3.419078 cop      lemma.lower():э\n",
      "3.147244 obl      word[-2:]:го\n",
      "3.126445 obj      word[-2:]:дү\n",
      "3.045331 ccomp    +1:lemma.lower():де\n",
      "2.908759 advcl    word[-2:]:ып\n",
      "2.840948 obj      word[-2:]:ды\n",
      "2.762383 _        lemma.lower():_\n",
      "2.739380 obl      word[-2:]:да\n",
      "2.692968 obl      word[-2:]:ге\n",
      "2.598093 advcl    word[-2:]:са\n",
      "2.460780 advcl    word[-2:]:ап\n",
      "2.424242 advcl    word[-2:]:еп\n",
      "2.415259 cc       postag:CCONJ\n",
      "2.407820 vocative 0_rus_deprel:vocative\n",
      "2.363789 punct    postag:PUNCT\n",
      "2.344148 obl      word[-2:]:ка\n",
      "2.316544 obl      word[-3:]:ден\n",
      "2.283101 nmod:poss word[-2:]:ын\n",
      "2.229121 acl      word[-2:]:ан\n",
      "2.170252 ccomp    +2:lemma.lower():де\n",
      "2.157551 nsubj    postag:PROPN\n",
      "2.149154 advcl    word[-2:]:уп\n",
      "\n",
      "Top negative:\n",
      "-1.012278 parataxis -1:postag:PRON\n",
      "-1.026087 ccomp    +2:lemma.lower():.\n",
      "-1.027631 parataxis +1:postag:NOUN\n",
      "-1.037312 ccomp    +2:word.lower():.\n",
      "-1.045975 conj     +1:postag:VERB\n",
      "-1.058044 root     +2:word.lower():,\n",
      "-1.065071 obj      +1:postag:ADJ\n",
      "-1.086244 nsubj    word[-2:]:де\n",
      "-1.097340 ccomp    -1:lemma.lower():—\n",
      "-1.119647 nmod:poss +1:postag:PRON\n",
      "-1.120869 parataxis +2:postag:VERB\n",
      "-1.139787 cop      lemma.lower() == word.lower()\n",
      "-1.145216 parataxis word[-2:]:ып\n",
      "-1.160168 advmod   -2:lemma.lower():бол\n",
      "-1.166870 nmod:poss lemma.lower() == word.lower()\n",
      "-1.206722 obl      word[-2:]:ун\n",
      "-1.258276 nmod:poss postag:VERB\n",
      "-1.274312 nmod:poss 2_rus_deprel:conj\n",
      "-1.335007 acl      lemma.lower() == word.lower()\n",
      "-1.426966 conj     -2:lemma.lower():бала\n",
      "-1.427413 parataxis +1:postag:VERB\n",
      "-1.456670 nsubj    word[-2:]:ын\n",
      "-1.460563 obj      postag:VERB\n",
      "-1.460951 nmod:poss +1:postag:VERB\n",
      "-1.554761 nsubj    postag:VERB\n",
      "-1.727112 amod     +1:postag:VERB\n",
      "-1.761086 obj      word[-2:]:ан\n",
      "-1.876982 punct    len(word)\n",
      "-1.891774 obl      postag:VERB\n",
      "-2.088548 obl      lemma.lower() == word.lower()\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92fee6a",
   "metadata": {},
   "source": [
    "# Перенос синтаксической разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce01e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "uas_score = []\n",
    "las_score = []\n",
    "clausal_tags = ['root', 'parataxis', 'csubj', 'xcomp', 'ccomp', 'advcl', 'acl', 'conj']\n",
    "\n",
    "for i in test_sents_id:\n",
    "\n",
    "    sentenizer = Synthesized_sent(map_ky_ru[i]['ru'], map_ky_ru[i]['ky'])\n",
    "    sentenizer.fill_deprel()\n",
    "\n",
    "    root = sentenizer.sent_source.to_tree()\n",
    "    sentenizer.parse_rus_sent(root)\n",
    "    sentenizer.add_dep_to_target()\n",
    "    sentenizer.head_renumerate()\n",
    "\n",
    "    sentenizer.fill_gapes()\n",
    "    uas, las = sentenizer.cal_uas()\n",
    "    uas_score.append(uas)\n",
    "    las_score.append(las)\n",
    "#     print(sentenizer.sent_target.serialize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c81d837b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46169246710687056, 0.07856310078045521)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UAS and LAS scores\n",
    "\n",
    "np.mean(uas_score), np.mean(las_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33751bdf",
   "metadata": {},
   "source": [
    "# Эксперименты со строковым разделением. Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f72b54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "\n",
    "    word = sent[i][0]\n",
    "    lemma = sent[i][1]\n",
    "    postag = sent[i][2]\n",
    "    set_deprel = [x['deprel'] for x in sent[i][4]]\n",
    "    id_row = sent[i][5]\n",
    "    rus_deprel = [x['deprel'] for x in sent[i][4].filter(upos = postag)]\n",
    "            \n",
    "    features = {\n",
    "#         'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'len(word)': len(word),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'postag': postag,\n",
    "        'lemma.lower()': lemma.lower(),\n",
    "#         'id_row': id_row,\n",
    "        'id_word_in_row': i,\n",
    "    }\n",
    "    \n",
    "\n",
    "    feat_transl = {str(i)+'_rus_deprel': x for i, x in enumerate(rus_deprel)}\n",
    "    features.update(feat_transl)\n",
    "    \n",
    "    if i > 1:\n",
    "        word1 = sent[i-2][0]\n",
    "        lemma1 = sent[i-2][1]\n",
    "        postag1 = sent[i-2][2]\n",
    "        features.update({\n",
    "            '-2:word.lower()': word1.lower(),\n",
    "            '-2:len(word)': len(word1),\n",
    "            '-2:postag': postag1,\n",
    "            '-2:lemma.lower()': lemma1.lower(),\n",
    "            '-2:id_word_in_row': i})\n",
    "    \n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        lemma1 = sent[i-1][1]\n",
    "        postag1 = sent[i-1][2]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:len(word)': len(word1),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:lemma.lower()': lemma1.lower(),\n",
    "            '-1:id_word_in_row': i\n",
    "#             '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "#     else:\n",
    "#         features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-2:\n",
    "        word1 = sent[i+2][0]\n",
    "        lemma1 = sent[i+2][1]\n",
    "        postag1 = sent[i+2][2]\n",
    "        features.update({\n",
    "            '+2:word.lower()': word1.lower(),\n",
    "            '+2:len(word)': len(word1),\n",
    "            '+2:postag': postag1,\n",
    "            '+2:lemma.lower()' : lemma1.lower(),\n",
    "            '+2:id_word_in_row': i})\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        lemma1 = sent[i+1][1]\n",
    "        postag1 = sent[i+1][2]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '-1:len(word)': len(word1),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:lemma.lower()' : lemma1.lower(),\n",
    "            '+1:id_word_in_row': i\n",
    "\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, lemma, postag, label, set_deprel, id_row in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, lemma, postag, label, set_deprel, id_row in sent]\n",
    "\n",
    "\n",
    "\n",
    "def trans2format_rows(sent_ky, sent_ru, i_row):\n",
    "    new_sent = []   \n",
    "\n",
    "    for word in sent_ky.filter(id=lambda x: type(x) is int):\n",
    "        new_sent.append((word['form'], word['lemma'], word['upos'], word['deprel'], sent_ru, i_row))\n",
    "        \n",
    "    return new_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c4aff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|████████████████████████████████████████| 1226/1226 [00:00<00:00, 15919.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 26412\n",
      "Seconds required: 0.042\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.03  loss=18001.20 active=26115 feature_norm=1.00\n",
      "Iter 2   time=0.01  loss=13659.98 active=26018 feature_norm=0.78\n",
      "Iter 3   time=0.01  loss=12936.63 active=25881 feature_norm=0.69\n",
      "Iter 4   time=0.01  loss=12350.09 active=26226 feature_norm=0.82\n",
      "Iter 5   time=0.01  loss=11935.93 active=26121 feature_norm=0.99\n",
      "Iter 6   time=0.01  loss=11265.23 active=25703 feature_norm=1.35\n",
      "Iter 7   time=0.01  loss=10767.31 active=25923 feature_norm=1.78\n",
      "Iter 8   time=0.01  loss=10279.56 active=26048 feature_norm=2.31\n",
      "Iter 9   time=0.02  loss=9639.45  active=26066 feature_norm=3.27\n",
      "Iter 10  time=0.01  loss=8607.66  active=25607 feature_norm=5.85\n",
      "Iter 11  time=0.01  loss=7682.95  active=25615 feature_norm=8.40\n",
      "Iter 12  time=0.02  loss=6869.19  active=25484 feature_norm=10.93\n",
      "Iter 13  time=0.01  loss=6124.15  active=24825 feature_norm=13.45\n",
      "Iter 14  time=0.01  loss=5604.33  active=24788 feature_norm=15.59\n",
      "Iter 15  time=0.02  loss=5083.97  active=24573 feature_norm=17.74\n",
      "Iter 16  time=0.01  loss=4688.97  active=24117 feature_norm=20.68\n",
      "Iter 17  time=0.02  loss=4297.06  active=23976 feature_norm=22.31\n",
      "Iter 18  time=0.02  loss=4034.76  active=23941 feature_norm=24.08\n",
      "Iter 19  time=0.01  loss=3680.21  active=23183 feature_norm=27.71\n",
      "Iter 20  time=0.02  loss=3358.92  active=23240 feature_norm=30.55\n",
      "Iter 21  time=0.01  loss=3070.74  active=22875 feature_norm=33.85\n",
      "Iter 22  time=0.02  loss=2782.54  active=22973 feature_norm=40.29\n",
      "Iter 23  time=0.01  loss=2521.95  active=22726 feature_norm=42.90\n",
      "Iter 24  time=0.02  loss=2319.10  active=22734 feature_norm=46.02\n",
      "Iter 25  time=0.01  loss=2119.89  active=21588 feature_norm=54.53\n",
      "Iter 26  time=0.02  loss=1956.28  active=21539 feature_norm=56.19\n",
      "Iter 27  time=0.02  loss=1902.17  active=21467 feature_norm=56.76\n",
      "Iter 28  time=0.02  loss=1805.56  active=21444 feature_norm=58.65\n",
      "Iter 29  time=0.01  loss=1748.27  active=21323 feature_norm=59.65\n",
      "Iter 30  time=0.01  loss=1746.39  active=20384 feature_norm=60.50\n",
      "Iter 31  time=0.02  loss=1671.24  active=20542 feature_norm=61.01\n",
      "Iter 32  time=0.01  loss=1663.07  active=20562 feature_norm=60.99\n",
      "Iter 33  time=0.01  loss=1644.55  active=20309 feature_norm=60.93\n",
      "Iter 34  time=0.02  loss=1625.62  active=19455 feature_norm=61.40\n",
      "Iter 35  time=0.01  loss=1604.15  active=19544 feature_norm=61.51\n",
      "Iter 36  time=0.02  loss=1600.63  active=19665 feature_norm=61.52\n",
      "Iter 37  time=0.01  loss=1595.78  active=19609 feature_norm=61.60\n",
      "Iter 38  time=0.02  loss=1584.74  active=19464 feature_norm=61.89\n",
      "Iter 39  time=0.01  loss=1579.85  active=19124 feature_norm=62.67\n",
      "Iter 40  time=0.02  loss=1564.38  active=19207 feature_norm=62.74\n",
      "Iter 41  time=0.01  loss=1561.22  active=19092 feature_norm=62.81\n",
      "Iter 42  time=0.02  loss=1550.66  active=18666 feature_norm=63.13\n",
      "Iter 43  time=0.04  loss=1547.57  active=18502 feature_norm=63.18\n",
      "Iter 44  time=0.01  loss=1539.19  active=18340 feature_norm=63.33\n",
      "Iter 45  time=0.02  loss=1531.05  active=18051 feature_norm=63.50\n",
      "Iter 46  time=0.02  loss=1526.83  active=17488 feature_norm=63.75\n",
      "Iter 47  time=0.02  loss=1517.64  active=17494 feature_norm=63.78\n",
      "Iter 48  time=0.02  loss=1514.42  active=17410 feature_norm=63.80\n",
      "Iter 49  time=0.02  loss=1506.99  active=17063 feature_norm=63.82\n",
      "Iter 50  time=0.02  loss=1505.44  active=16774 feature_norm=63.90\n",
      "Iter 51  time=0.02  loss=1498.89  active=16831 feature_norm=63.85\n",
      "Iter 52  time=0.02  loss=1496.13  active=16743 feature_norm=63.81\n",
      "Iter 53  time=0.02  loss=1490.81  active=16471 feature_norm=63.79\n",
      "Iter 54  time=0.03  loss=1488.62  active=16441 feature_norm=63.77\n",
      "Iter 55  time=0.01  loss=1486.08  active=16394 feature_norm=63.75\n",
      "Iter 56  time=0.02  loss=1482.90  active=16232 feature_norm=63.75\n",
      "Iter 57  time=0.01  loss=1481.60  active=16006 feature_norm=63.74\n",
      "Iter 58  time=0.02  loss=1478.57  active=16013 feature_norm=63.74\n",
      "Iter 59  time=0.01  loss=1477.29  active=15962 feature_norm=63.73\n",
      "Iter 60  time=0.02  loss=1474.91  active=15808 feature_norm=63.69\n",
      "Iter 61  time=0.01  loss=1472.93  active=15717 feature_norm=63.67\n",
      "Iter 62  time=0.02  loss=1471.28  active=15675 feature_norm=63.64\n",
      "Iter 63  time=0.01  loss=1469.30  active=15537 feature_norm=63.60\n",
      "Iter 64  time=0.02  loss=1468.08  active=15358 feature_norm=63.57\n",
      "Iter 65  time=0.01  loss=1466.24  active=15365 feature_norm=63.56\n",
      "Iter 66  time=0.02  loss=1465.23  active=15326 feature_norm=63.55\n",
      "Iter 67  time=0.01  loss=1463.37  active=15146 feature_norm=63.52\n",
      "Iter 68  time=0.02  loss=1462.32  active=15157 feature_norm=63.52\n",
      "Iter 69  time=0.02  loss=1461.27  active=15177 feature_norm=63.51\n",
      "Iter 70  time=0.02  loss=1460.30  active=15127 feature_norm=63.49\n",
      "Iter 71  time=0.02  loss=1459.42  active=15057 feature_norm=63.48\n",
      "Iter 72  time=0.01  loss=1458.10  active=15066 feature_norm=63.47\n",
      "Iter 73  time=0.02  loss=1457.38  active=15038 feature_norm=63.47\n",
      "Iter 74  time=0.02  loss=1456.53  active=14979 feature_norm=63.45\n",
      "Iter 75  time=0.02  loss=1455.66  active=14948 feature_norm=63.44\n",
      "Iter 76  time=0.02  loss=1454.85  active=14933 feature_norm=63.42\n",
      "Iter 77  time=0.01  loss=1454.09  active=14905 feature_norm=63.42\n",
      "Iter 78  time=0.02  loss=1453.41  active=14854 feature_norm=63.41\n",
      "Iter 79  time=0.02  loss=1452.74  active=14828 feature_norm=63.41\n",
      "Iter 80  time=0.01  loss=1452.16  active=14813 feature_norm=63.41\n",
      "Iter 81  time=0.02  loss=1451.50  active=14778 feature_norm=63.41\n",
      "Iter 82  time=0.02  loss=1450.92  active=14721 feature_norm=63.42\n",
      "Iter 83  time=0.02  loss=1450.33  active=14689 feature_norm=63.43\n",
      "Iter 84  time=0.02  loss=1449.87  active=14673 feature_norm=63.43\n",
      "Iter 85  time=0.01  loss=1449.39  active=14653 feature_norm=63.44\n",
      "Iter 86  time=0.02  loss=1448.94  active=14614 feature_norm=63.44\n",
      "Iter 87  time=0.02  loss=1448.45  active=14588 feature_norm=63.46\n",
      "Iter 88  time=0.02  loss=1448.01  active=14588 feature_norm=63.46\n",
      "Iter 89  time=0.02  loss=1447.63  active=14584 feature_norm=63.47\n",
      "Iter 90  time=0.02  loss=1447.27  active=14582 feature_norm=63.47\n",
      "Iter 91  time=0.01  loss=1446.88  active=14534 feature_norm=63.48\n",
      "Iter 92  time=0.02  loss=1446.54  active=14494 feature_norm=63.48\n",
      "Iter 93  time=0.02  loss=1446.24  active=14475 feature_norm=63.49\n",
      "Iter 94  time=0.02  loss=1445.91  active=14460 feature_norm=63.49\n",
      "Iter 95  time=0.02  loss=1445.66  active=14428 feature_norm=63.50\n",
      "Iter 96  time=0.01  loss=1445.37  active=14428 feature_norm=63.51\n",
      "Iter 97  time=0.02  loss=1445.14  active=14388 feature_norm=63.51\n",
      "Iter 98  time=0.02  loss=1444.85  active=14379 feature_norm=63.52\n",
      "Iter 99  time=0.02  loss=1444.65  active=14375 feature_norm=63.52\n",
      "Iter 100 time=0.01  loss=1444.39  active=14361 feature_norm=63.53\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 1.624\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 14361 (26412)\n",
      "Number of active attributes: 8308 (13980)\n",
      "Number of active labels: 29 (29)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.010\n",
      "\n",
      "F1-score is : 69.4%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       1.00      1.00      1.00         6\n",
      "         acl       0.77      0.66      0.71        35\n",
      "       advcl       0.75      0.81      0.78        81\n",
      "      advmod       0.93      0.88      0.90        48\n",
      " advmod:emph       1.00      1.00      1.00         8\n",
      "        amod       0.70      0.93      0.80        30\n",
      "       appos       0.88      0.47      0.61        15\n",
      "         aux       0.87      0.83      0.85        41\n",
      "        case       0.83      0.77      0.80        13\n",
      "          cc       1.00      1.00      1.00         4\n",
      "       ccomp       0.80      0.80      0.80        25\n",
      "    compound       0.60      0.47      0.53        19\n",
      "        conj       0.70      0.54      0.61        72\n",
      "         cop       0.90      1.00      0.95         9\n",
      "         det       1.00      0.69      0.82        13\n",
      "   discourse       1.00      0.75      0.86         4\n",
      "       fixed       0.00      0.00      0.00         3\n",
      "        flat       0.00      0.00      0.00         1\n",
      "        nmod       0.47      0.35      0.40        23\n",
      "   nmod:poss       0.52      0.71      0.60        17\n",
      "       nsubj       0.69      0.80      0.74       103\n",
      "      nummod       0.89      0.84      0.86        19\n",
      "         obj       0.72      0.68      0.70        85\n",
      "         obl       0.84      0.85      0.85        81\n",
      "   parataxis       0.45      0.48      0.46        52\n",
      "       punct       1.00      1.00      1.00       239\n",
      "        root       0.77      0.84      0.81        94\n",
      "    vocative       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.80      0.79      0.79      1143\n",
      "   macro avg       0.72      0.68      0.69      1143\n",
      "weighted avg       0.80      0.79      0.79      1143\n",
      " samples avg       0.80      0.79      0.79      1143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['obl',\n",
       " 'root',\n",
       " 'nsubj',\n",
       " 'appos',\n",
       " 'punct',\n",
       " 'conj',\n",
       " 'advmod',\n",
       " 'advcl',\n",
       " 'ccomp',\n",
       " 'obj',\n",
       " 'amod',\n",
       " 'nmod',\n",
       " 'det',\n",
       " 'vocative',\n",
       " 'parataxis',\n",
       " 'nmod:poss',\n",
       " '_',\n",
       " 'aux',\n",
       " 'compound',\n",
       " 'advmod:emph',\n",
       " 'nummod',\n",
       " 'acl',\n",
       " 'cc',\n",
       " 'case',\n",
       " 'xcomp',\n",
       " 'cop',\n",
       " 'fixed',\n",
       " 'csubj',\n",
       " 'discourse']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = []\n",
    "test_sents = []\n",
    "\n",
    "for i, ky_ru_sent in map_ky_ru.items():\n",
    "    syntenizer = Synthesized_sent(map_ky_ru[i]['ru'], map_ky_ru[i]['ky'])\n",
    "    \n",
    "    for i_row, row in enumerate(syntenizer.rows_target):\n",
    "        if i in ['4059', '4647', '4698', '4858', '4902', '5064', '4729', '5452']:\n",
    "            pass\n",
    "        elif i not in test_sents_id:\n",
    "#             print(i)\n",
    "            train_sents.append((trans2format_rows(row, syntenizer.rows_source[i_row], i_row)))\n",
    "        else:\n",
    "            \n",
    "            test_sents.append((trans2format_rows(row, syntenizer.rows_source[i_row], i_row)))\n",
    "\n",
    "            \n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "\n",
    "m = MultiLabelBinarizer().fit(y_test)\n",
    "\n",
    "print(\"F1-score is : {:.1%}\".format(f1_score(m.transform(y_test),\n",
    "         m.transform(y_pred),\n",
    "         average='macro')))\n",
    "print(classification_report(m.transform(y_test), m.transform(y_pred), target_names=m.classes_))\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04615b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|████████████████████████████████████████| 1226/1226 [00:00<00:00, 15521.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 26412\n",
      "Seconds required: 0.042\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.138868\n",
      "c2: 0.014639\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.03  loss=18002.07 active=25919 feature_norm=1.00\n",
      "Iter 2   time=0.01  loss=13660.57 active=26041 feature_norm=0.78\n",
      "Iter 3   time=0.02  loss=12937.55 active=25854 feature_norm=0.69\n",
      "Iter 4   time=0.01  loss=12351.07 active=26187 feature_norm=0.82\n",
      "Iter 5   time=0.02  loss=11937.31 active=26096 feature_norm=0.99\n",
      "Iter 6   time=0.01  loss=11267.43 active=25624 feature_norm=1.35\n",
      "Iter 7   time=0.02  loss=10770.48 active=25836 feature_norm=1.78\n",
      "Iter 8   time=0.02  loss=10284.26 active=25914 feature_norm=2.31\n",
      "Iter 9   time=0.02  loss=9646.58  active=25929 feature_norm=3.26\n",
      "Iter 10  time=0.02  loss=8619.12  active=25100 feature_norm=5.83\n",
      "Iter 11  time=0.02  loss=7696.45  active=25036 feature_norm=8.38\n",
      "Iter 12  time=0.02  loss=6882.19  active=24976 feature_norm=10.91\n",
      "Iter 13  time=0.02  loss=6135.17  active=24677 feature_norm=14.28\n",
      "Iter 14  time=0.02  loss=5618.45  active=24523 feature_norm=15.86\n",
      "Iter 15  time=0.02  loss=5095.35  active=24259 feature_norm=18.00\n",
      "Iter 16  time=0.02  loss=4728.38  active=23897 feature_norm=21.03\n",
      "Iter 17  time=0.02  loss=4337.34  active=23807 feature_norm=22.45\n",
      "Iter 18  time=0.02  loss=4095.93  active=23784 feature_norm=24.01\n",
      "Iter 19  time=0.02  loss=3726.57  active=23193 feature_norm=27.24\n",
      "Iter 20  time=0.02  loss=3376.68  active=22943 feature_norm=31.21\n",
      "Iter 21  time=0.02  loss=3065.09  active=22802 feature_norm=34.89\n",
      "Iter 22  time=0.02  loss=2802.96  active=22645 feature_norm=39.20\n",
      "Iter 23  time=0.02  loss=2515.16  active=22429 feature_norm=44.05\n",
      "Iter 24  time=0.02  loss=2220.09  active=21649 feature_norm=50.29\n",
      "Iter 25  time=0.02  loss=1976.40  active=20605 feature_norm=57.04\n",
      "Iter 26  time=0.02  loss=1821.32  active=20681 feature_norm=60.80\n",
      "Iter 27  time=0.02  loss=1691.60  active=20397 feature_norm=64.94\n",
      "Iter 28  time=0.03  loss=1628.97  active=20285 feature_norm=67.45\n",
      "Iter 29  time=0.02  loss=1558.53  active=19833 feature_norm=69.24\n",
      "Iter 30  time=0.02  loss=1511.90  active=19684 feature_norm=70.50\n",
      "Iter 31  time=0.02  loss=1474.31  active=19063 feature_norm=71.94\n",
      "Iter 32  time=0.02  loss=1444.44  active=18931 feature_norm=72.59\n",
      "Iter 33  time=0.02  loss=1430.30  active=18485 feature_norm=72.78\n",
      "Iter 34  time=0.01  loss=1407.10  active=17134 feature_norm=73.62\n",
      "Iter 35  time=0.02  loss=1398.18  active=16940 feature_norm=74.29\n",
      "Iter 36  time=0.01  loss=1386.38  active=16927 feature_norm=74.53\n",
      "Iter 37  time=0.02  loss=1376.86  active=16696 feature_norm=75.07\n",
      "Iter 38  time=0.02  loss=1363.71  active=16108 feature_norm=75.95\n",
      "Iter 39  time=0.02  loss=1362.91  active=15552 feature_norm=77.65\n",
      "Iter 40  time=0.02  loss=1346.13  active=15579 feature_norm=77.98\n",
      "Iter 41  time=0.02  loss=1343.05  active=15506 feature_norm=78.15\n",
      "Iter 42  time=0.02  loss=1331.00  active=14514 feature_norm=79.82\n",
      "Iter 43  time=0.03  loss=1330.62  active=14461 feature_norm=79.89\n",
      "Iter 44  time=0.02  loss=1326.33  active=14585 feature_norm=79.81\n",
      "Iter 45  time=0.02  loss=1324.54  active=14485 feature_norm=79.82\n",
      "Iter 46  time=0.02  loss=1320.50  active=14200 feature_norm=79.89\n",
      "Iter 47  time=0.03  loss=1318.68  active=13799 feature_norm=79.96\n",
      "Iter 48  time=0.02  loss=1311.89  active=13663 feature_norm=80.13\n",
      "Iter 49  time=0.02  loss=1308.77  active=13522 feature_norm=80.20\n",
      "Iter 50  time=0.02  loss=1303.33  active=13106 feature_norm=80.45\n",
      "Iter 51  time=0.02  loss=1299.19  active=13011 feature_norm=80.55\n",
      "Iter 52  time=0.02  loss=1295.85  active=12861 feature_norm=80.63\n",
      "Iter 53  time=0.02  loss=1289.59  active=12406 feature_norm=80.92\n",
      "Iter 54  time=0.03  loss=1287.83  active=12279 feature_norm=81.05\n",
      "Iter 55  time=0.02  loss=1284.41  active=12220 feature_norm=81.15\n",
      "Iter 56  time=0.02  loss=1281.32  active=12014 feature_norm=81.30\n",
      "Iter 57  time=0.02  loss=1277.84  active=11721 feature_norm=81.62\n",
      "Iter 58  time=0.02  loss=1274.56  active=11641 feature_norm=81.75\n",
      "Iter 59  time=0.02  loss=1272.48  active=11622 feature_norm=81.89\n",
      "Iter 60  time=0.02  loss=1269.06  active=11487 feature_norm=82.09\n",
      "Iter 61  time=0.02  loss=1266.40  active=11369 feature_norm=82.24\n",
      "Iter 62  time=0.02  loss=1263.97  active=11301 feature_norm=82.36\n",
      "Iter 63  time=0.02  loss=1261.83  active=11189 feature_norm=82.47\n",
      "Iter 64  time=0.02  loss=1259.58  active=11072 feature_norm=82.60\n",
      "Iter 65  time=0.02  loss=1257.46  active=11002 feature_norm=82.73\n",
      "Iter 66  time=0.02  loss=1255.48  active=10902 feature_norm=82.84\n",
      "Iter 67  time=0.02  loss=1253.67  active=10867 feature_norm=82.98\n",
      "Iter 68  time=0.02  loss=1252.11  active=10855 feature_norm=83.05\n",
      "Iter 69  time=0.02  loss=1250.49  active=10779 feature_norm=83.13\n",
      "Iter 70  time=0.02  loss=1248.82  active=10664 feature_norm=83.23\n",
      "Iter 71  time=0.02  loss=1247.15  active=10636 feature_norm=83.28\n",
      "Iter 72  time=0.02  loss=1245.83  active=10596 feature_norm=83.33\n",
      "Iter 73  time=0.02  loss=1244.05  active=10539 feature_norm=83.39\n",
      "Iter 74  time=0.02  loss=1242.49  active=10468 feature_norm=83.47\n",
      "Iter 75  time=0.02  loss=1241.36  active=10451 feature_norm=83.54\n",
      "Iter 76  time=0.02  loss=1240.25  active=10440 feature_norm=83.60\n",
      "Iter 77  time=0.02  loss=1239.00  active=10349 feature_norm=83.66\n",
      "Iter 78  time=0.02  loss=1237.79  active=10244 feature_norm=83.75\n",
      "Iter 79  time=0.02  loss=1236.82  active=10204 feature_norm=83.80\n",
      "Iter 80  time=0.02  loss=1235.98  active=10161 feature_norm=83.83\n",
      "Iter 81  time=0.02  loss=1234.89  active=10083 feature_norm=83.87\n",
      "Iter 82  time=0.02  loss=1234.07  active=10012 feature_norm=83.95\n",
      "Iter 83  time=0.02  loss=1233.08  active=10014 feature_norm=84.01\n",
      "Iter 84  time=0.02  loss=1232.51  active=10001 feature_norm=84.04\n",
      "Iter 85  time=0.02  loss=1231.75  active=9949  feature_norm=84.12\n",
      "Iter 86  time=0.02  loss=1231.10  active=9910  feature_norm=84.19\n",
      "Iter 87  time=0.02  loss=1230.28  active=9890  feature_norm=84.28\n",
      "Iter 88  time=0.02  loss=1229.67  active=9868  feature_norm=84.33\n",
      "Iter 89  time=0.02  loss=1229.06  active=9824  feature_norm=84.42\n",
      "Iter 90  time=0.02  loss=1228.50  active=9784  feature_norm=84.50\n",
      "Iter 91  time=0.02  loss=1227.90  active=9768  feature_norm=84.59\n",
      "Iter 92  time=0.02  loss=1227.35  active=9742  feature_norm=84.65\n",
      "Iter 93  time=0.02  loss=1226.90  active=9703  feature_norm=84.72\n",
      "Iter 94  time=0.02  loss=1226.43  active=9671  feature_norm=84.79\n",
      "Iter 95  time=0.02  loss=1225.97  active=9636  feature_norm=84.86\n",
      "Iter 96  time=0.02  loss=1225.48  active=9600  feature_norm=84.92\n",
      "Iter 97  time=0.02  loss=1225.09  active=9580  feature_norm=84.99\n",
      "Iter 98  time=0.02  loss=1224.68  active=9569  feature_norm=85.05\n",
      "Iter 99  time=0.02  loss=1224.27  active=9535  feature_norm=85.11\n",
      "Iter 100 time=0.02  loss=1223.91  active=9511  feature_norm=85.18\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 1.694\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 9511 (26412)\n",
      "Number of active attributes: 5919 (13980)\n",
      "Number of active labels: 29 (29)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=True,\n",
    "    keep_tempfiles=None\n",
    ")\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "predictions = rs.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c110e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score is : 68.8%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       1.00      1.00      1.00         6\n",
      "         acl       0.80      0.69      0.74        35\n",
      "       advcl       0.72      0.80      0.76        81\n",
      "      advmod       0.93      0.88      0.90        48\n",
      " advmod:emph       1.00      1.00      1.00         8\n",
      "        amod       0.72      0.93      0.81        30\n",
      "       appos       0.78      0.47      0.58        15\n",
      "         aux       0.87      0.83      0.85        41\n",
      "        case       0.83      0.77      0.80        13\n",
      "          cc       1.00      1.00      1.00         4\n",
      "       ccomp       0.83      0.76      0.79        25\n",
      "    compound       0.56      0.47      0.51        19\n",
      "        conj       0.60      0.47      0.53        72\n",
      "         cop       0.90      1.00      0.95         9\n",
      "         det       1.00      0.69      0.82        13\n",
      "   discourse       1.00      0.75      0.86         4\n",
      "       fixed       0.00      0.00      0.00         3\n",
      "        flat       0.00      0.00      0.00         1\n",
      "        nmod       0.42      0.22      0.29        23\n",
      "   nmod:poss       0.48      0.76      0.59        17\n",
      "       nsubj       0.68      0.81      0.74       103\n",
      "      nummod       0.89      0.84      0.86        19\n",
      "         obj       0.75      0.67      0.71        85\n",
      "         obl       0.86      0.85      0.86        81\n",
      "   parataxis       0.48      0.52      0.50        52\n",
      "       punct       1.00      1.00      1.00       239\n",
      "        root       0.79      0.84      0.81        94\n",
      "    vocative       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1143\n",
      "   macro avg       0.71      0.68      0.69      1143\n",
      "weighted avg       0.79      0.79      0.79      1143\n",
      " samples avg       0.79      0.78      0.78      1143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "m = MultiLabelBinarizer().fit(y_test)\n",
    "\n",
    "print(\"F1-score is : {:.1%}\".format(f1_score(m.transform(y_test),\n",
    "         m.transform(y_pred),\n",
    "         average='macro')))\n",
    "print(classification_report(m.transform(y_test), m.transform(y_pred), target_names=m.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "133936c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "8.493134 det      postag:DET\n",
      "8.332186 _        postag:_\n",
      "7.708944 aux      postag:AUX\n",
      "6.954878 cop      lemma.lower():э\n",
      "6.648028 advmod   postag:ADV\n",
      "5.965671 case     postag:ADP\n",
      "5.939333 amod     postag:ADJ\n",
      "5.807218 nummod   postag:NUM\n",
      "5.632895 cc       postag:CCONJ\n",
      "5.318930 obl      word[-2:]:го\n",
      "5.222256 punct    postag:PUNCT\n",
      "4.975901 punct    0_rus_deprel:punct\n",
      "4.917686 ccomp    +1:lemma.lower():де\n",
      "4.915084 obl      word[-3:]:ден\n",
      "4.879851 vocative 0_rus_deprel:vocative\n",
      "4.848275 compound +1:lemma.lower():мылтык\n",
      "4.753372 obl      word[-2:]:га\n",
      "4.737401 obj      word[-2:]:дү\n",
      "4.613810 compound +1:word.lower():бери\n",
      "4.509783 conj     word.lower():кирейин\n",
      "4.476326 parataxis lemma.lower():каш\n",
      "4.420366 ccomp    word.lower():кебиңди\n",
      "4.380569 obl      word[-2:]:ге\n",
      "4.233163 obl      word[-2:]:да\n",
      "4.148691 advcl    +1:word.lower():албай\n",
      "4.139104 obj      word.lower():аны\n",
      "4.132528 vocative lemma.lower():арбак\n",
      "4.081854 advcl    word[-2:]:са\n",
      "4.071219 compound -1:word.lower():чабдарга\n",
      "3.979285 root     word.lower():турушат\n",
      "\n",
      "Top negative:\n",
      "-1.638559 appos    +2:postag:PUNCT\n",
      "-1.642418 conj     -2:postag:PRON\n",
      "-1.645189 nmod:poss +1:postag:VERB\n",
      "-1.654426 nsubj    word[-2:]:ын\n",
      "-1.672523 nummod   +1:word.lower():күн\n",
      "-1.686348 obl      +1:lemma.lower():бер\n",
      "-1.694252 obj      lemma.lower():күн\n",
      "-1.711029 nmod     0_rus_deprel:parataxis\n",
      "-1.728136 advcl    -2:postag:ADP\n",
      "-1.748430 obl      +1:word.lower():манас\n",
      "-1.772898 root     -2:lemma.lower():де\n",
      "-1.786115 root     lemma.lower():жүр\n",
      "-1.787093 acl      +2:postag:PUNCT\n",
      "-1.801533 amod     0_rus_deprel:obl\n",
      "-1.803761 advmod   word[-2:]:ып\n",
      "-1.813044 obl      +2:word.lower():калганы\n",
      "-1.844096 nmod     1_rus_deprel:nsubj\n",
      "-1.861940 parataxis +1:postag:VERB\n",
      "-1.979230 parataxis -2:lemma.lower():бир\n",
      "-1.990108 nmod:poss +1:postag:PRON\n",
      "-2.051839 nmod:poss postag:VERB\n",
      "-2.163587 ccomp    lemma.lower():де\n",
      "-2.251677 obj      postag:VERB\n",
      "-2.278344 advmod   word[-3:]:тан\n",
      "-2.444858 conj     -2:lemma.lower():бала\n",
      "-2.456134 amod     +1:postag:VERB\n",
      "-2.456402 obj      word[-2:]:ан\n",
      "-2.663389 nsubj    postag:VERB\n",
      "-3.069885 punct    len(word)\n",
      "-3.568736 obl      postag:VERB\n"
     ]
    }
   ],
   "source": [
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6d487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
